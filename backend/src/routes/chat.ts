import express from 'express';
import { v4 as uuidv4 } from 'uuid';

const router = express.Router();

// Mock AI response generator
const generateMockAIResponse = (userMessage: string): string => {
  const responses = [
    'ãã‚Œã¯èˆˆå‘³æ·±ã„è³ªå•ã§ã™ã­ã€‚è©³ã—ãæ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ',
    'ãªã‚‹ã»ã©ã€ã‚ˆãç†è§£ã§ãã¾ã—ãŸã€‚ä»–ã«ã‚‚ä½•ã‹ãŠèãã—ãŸã„ã“ã¨ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ',
    'ãã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯è‰¯ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã ã¨æ€ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã©ã®ã‚ˆã†ã«é€²ã‚ã¾ã™ã‹ï¼Ÿ',
    'ã¨ã¦ã‚‚è‰¯ã„è¦³ç‚¹ã§ã™ã­ã€‚åˆ¥ã®è¦–ç‚¹ã‹ã‚‰è€ƒãˆã¦ã¿ã‚‹ã¨...',
    'ãã‚Œã«ã¤ã„ã¦ã‚‚ã†å°‘ã—è©³ã—ãèª¬æ˜ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ',
    'ãã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®ã„ãã¤ã‹ã®æ–¹æ³•ã‚’ææ¡ˆã§ãã¾ã™ã€‚',
    'ã¨ã¦ã‚‚èˆˆå‘³æ·±ã„ãƒˆãƒ”ãƒƒã‚¯ã§ã™ã­ã€‚é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ãŠæ•™ãˆã—ã¾ã™ã€‚'
  ];
  
  // Simple response selection based on message content
  if (userMessage.includes('?') || userMessage.includes('ï¼Ÿ')) {
    return responses[Math.floor(Math.random() * responses.length)];
  }
  
  if (userMessage.includes('ã“ã‚“ã«ã¡ã¯') || userMessage.includes('ã¯ã˜ã‚ã¾ã—ã¦')) {
    return 'ã“ã‚“ã«ã¡ã¯ï¼ä»Šæ—¥ã¯ã©ã®ã‚ˆã†ãªã“ã¨ã‚’ãŠæ‰‹ä¼ã„ã§ãã¾ã™ã‹ï¼Ÿ';
  }
  
  if (userMessage.includes('ã‚ã‚ŠãŒã¨ã†')) {
    return 'ã©ã†ã„ãŸã—ã¾ã—ã¦ï¼ä»–ã«ã‚‚ä½•ã‹ãŠæ‰‹ä¼ã„ã§ãã‚‹ã“ã¨ãŒã‚ã‚Œã°ã€ãŠæ°—è»½ã«ãŠå£°ã‹ã‘ãã ã•ã„ã€‚';
  }
  
  return responses[Math.floor(Math.random() * responses.length)];
};

// POST /api/chat/stream - Handle chat messages
router.post('/stream', (req, res) => {
  console.log('ğŸ’¬ Processing chat message...');
  
  const { message, conversationId, model = 'gpt-4' } = req.body;
  
  if (!message) {
    return res.status(400).json({
      error: 'Bad Request',
      message: 'Message content is required'
    });
  }
  
  // Simulate AI processing delay
  setTimeout(() => {
    const aiResponse = generateMockAIResponse(message);
    const messageId = uuidv4();
    const now = new Date().toISOString();
    
    const response = {
      id: messageId,
      role: 'assistant',
      content: aiResponse,
      model: model,
      conversationId: conversationId || uuidv4(),
      timestamp: now,
      usage: {
        prompt_tokens: message.length,
        completion_tokens: aiResponse.length,
        total_tokens: message.length + aiResponse.length
      }
    };
    
    console.log(`âœ… AI response generated for conversation: ${response.conversationId}`);
    
    res.json({
      message: response,
      success: true
    });
  }, 500 + Math.random() * 1000); // Random delay 500-1500ms for realism
});

// POST /api/chat/models - Get available AI models
router.get('/models', (req, res) => {
  console.log('ğŸ¤– Fetching available AI models...');
  
  const models = [
    {
      id: 'gpt-4',
      name: 'GPT-4',
      provider: 'OpenAI',
      description: 'æœ€æ–°ã®é«˜æ€§èƒ½å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«',
      available: true
    },
    {
      id: 'gpt-3.5-turbo',
      name: 'GPT-3.5 Turbo',
      provider: 'OpenAI', 
      description: 'é«˜é€Ÿã§åŠ¹ç‡çš„ãªè¨€èªãƒ¢ãƒ‡ãƒ«',
      available: true
    },
    {
      id: 'claude-3-sonnet',
      name: 'Claude 3 Sonnet',
      provider: 'Anthropic',
      description: 'ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé«˜æ€§èƒ½ãƒ¢ãƒ‡ãƒ«',
      available: true
    },
    {
      id: 'claude-3-haiku',
      name: 'Claude 3 Haiku',
      provider: 'Anthropic',
      description: 'é«˜é€Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«',
      available: true
    }
  ];
  
  res.json({
    models,
    default: 'gpt-4'
  });
});

export { router as chatRouter };
